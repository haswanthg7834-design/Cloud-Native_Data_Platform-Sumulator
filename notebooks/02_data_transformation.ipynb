{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation Pipeline\n",
    "This notebook demonstrates data cleaning, transformation, and preparation for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "Load raw data from CSV files or database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    \"\"\"Load raw data from CSV files\"\"\"\n",
    "    data_path = '../data/raw/'\n",
    "    datasets = {}\n",
    "    \n",
    "    csv_files = ['customers.csv', 'transactions.csv', 'events.csv', 'products.csv']\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            dataset_name = file.replace('.csv', '')\n",
    "            datasets[dataset_name] = pd.read_csv(file_path)\n",
    "            logger.info(f\"Loaded {dataset_name}: {len(datasets[dataset_name])} records\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "raw_data = load_raw_data()\n",
    "print(\"Raw data loaded successfully!\")\n",
    "for name, df in raw_data.items():\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "Handle missing values, duplicates, and data type conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customers_data(df):\n",
    "    \"\"\"Clean customer data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns\n",
    "    df_clean['registration_date'] = pd.to_datetime(df_clean['registration_date'])\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean['phone'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Standardize categorical data\n",
    "    df_clean['state'] = df_clean['state'].str.upper()\n",
    "    df_clean['segment'] = df_clean['segment'].str.title()\n",
    "    \n",
    "    # Create derived features\n",
    "    df_clean['days_since_registration'] = (datetime.now() - df_clean['registration_date']).dt.days\n",
    "    df_clean['age_group'] = pd.cut(df_clean['age'], bins=[0, 25, 35, 50, 65, 100], \n",
    "                                   labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_transactions_data(df):\n",
    "    \"\"\"Clean transaction data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns\n",
    "    df_clean['transaction_date'] = pd.to_datetime(df_clean['transaction_date'])\n",
    "    \n",
    "    # Remove failed transactions for analysis\n",
    "    df_clean = df_clean[df_clean['status'] != 'failed'].copy()\n",
    "    \n",
    "    # Create derived features\n",
    "    df_clean['transaction_month'] = df_clean['transaction_date'].dt.month\n",
    "    df_clean['transaction_day_of_week'] = df_clean['transaction_date'].dt.day_name()\n",
    "    df_clean['transaction_hour'] = df_clean['transaction_date'].dt.hour\n",
    "    \n",
    "    # Categorize amount ranges\n",
    "    df_clean['amount_category'] = pd.cut(df_clean['amount'], \n",
    "                                        bins=[0, 50, 200, 500, float('inf')],\n",
    "                                        labels=['Small', 'Medium', 'Large', 'Very Large'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_events_data(df):\n",
    "    \"\"\"Clean events data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert timestamp\n",
    "    df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "    \n",
    "    # Create derived features\n",
    "    df_clean['event_date'] = df_clean['timestamp'].dt.date\n",
    "    df_clean['event_hour'] = df_clean['timestamp'].dt.hour\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_products_data(df):\n",
    "    \"\"\"Clean products data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns\n",
    "    df_clean['created_date'] = pd.to_datetime(df_clean['created_date'])\n",
    "    \n",
    "    # Calculate profit margin\n",
    "    df_clean['profit_margin'] = (df_clean['price'] - df_clean['cost']) / df_clean['price'] * 100\n",
    "    \n",
    "    # Categorize stock levels\n",
    "    df_clean['stock_level'] = pd.cut(df_clean['stock_quantity'],\n",
    "                                    bins=[-1, 0, 50, 200, float('inf')],\n",
    "                                    labels=['Out of Stock', 'Low', 'Medium', 'High'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean all datasets\n",
    "cleaned_data = {}\n",
    "cleaned_data['customers'] = clean_customers_data(raw_data['customers'])\n",
    "cleaned_data['transactions'] = clean_transactions_data(raw_data['transactions'])\n",
    "cleaned_data['events'] = clean_events_data(raw_data['events'])\n",
    "cleaned_data['products'] = clean_products_data(raw_data['products'])\n",
    "\n",
    "print(\"Data cleaning completed!\")\n",
    "for name, df in cleaned_data.items():\n",
    "    print(f\"Cleaned {name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Integration\n",
    "Create integrated datasets by joining tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_transaction_summary():\n",
    "    \"\"\"Create customer transaction summary\"\"\"\n",
    "    customers = cleaned_data['customers']\n",
    "    transactions = cleaned_data['transactions']\n",
    "    \n",
    "    # Aggregate transaction data per customer\n",
    "    txn_summary = transactions.groupby('customer_id').agg({\n",
    "        'amount': ['count', 'sum', 'mean', 'std'],\n",
    "        'transaction_date': ['min', 'max'],\n",
    "        'currency': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'USD'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    txn_summary.columns = ['_'.join(col).strip() for col in txn_summary.columns]\n",
    "    txn_summary = txn_summary.rename(columns={\n",
    "        'amount_count': 'total_transactions',\n",
    "        'amount_sum': 'total_spent',\n",
    "        'amount_mean': 'avg_transaction_amount',\n",
    "        'amount_std': 'transaction_amount_std',\n",
    "        'transaction_date_min': 'first_transaction',\n",
    "        'transaction_date_max': 'last_transaction',\n",
    "        'currency_<lambda>': 'primary_currency'\n",
    "    })\n",
    "    \n",
    "    # Calculate days since last transaction\n",
    "    txn_summary['days_since_last_transaction'] = (\n",
    "        datetime.now() - pd.to_datetime(txn_summary['last_transaction'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # Join with customer data\n",
    "    customer_summary = customers.merge(txn_summary, on='customer_id', how='left')\n",
    "    \n",
    "    # Fill NaN values for customers with no transactions\n",
    "    txn_columns = ['total_transactions', 'total_spent', 'avg_transaction_amount']\n",
    "    customer_summary[txn_columns] = customer_summary[txn_columns].fillna(0)\n",
    "    \n",
    "    return customer_summary\n",
    "\n",
    "def create_daily_metrics():\n",
    "    \"\"\"Create daily business metrics\"\"\"\n",
    "    transactions = cleaned_data['transactions']\n",
    "    events = cleaned_data['events']\n",
    "    \n",
    "    # Daily transaction metrics\n",
    "    daily_txn = transactions.groupby(transactions['transaction_date'].dt.date).agg({\n",
    "        'amount': ['count', 'sum', 'mean'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    daily_txn.columns = ['daily_transactions', 'daily_revenue', 'avg_transaction_value', 'unique_customers']\n",
    "    daily_txn.reset_index(inplace=True)\n",
    "    daily_txn.rename(columns={'transaction_date': 'date'}, inplace=True)\n",
    "    \n",
    "    # Daily event metrics\n",
    "    daily_events = events.groupby(events['timestamp'].dt.date).agg({\n",
    "        'event_id': 'count',\n",
    "        'customer_id': 'nunique',\n",
    "        'session_id': 'nunique'\n",
    "    })\n",
    "    \n",
    "    daily_events.columns = ['daily_events', 'active_users', 'sessions']\n",
    "    daily_events.reset_index(inplace=True)\n",
    "    daily_events.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "    \n",
    "    # Merge metrics\n",
    "    daily_metrics = daily_txn.merge(daily_events, on='date', how='outer').fillna(0)\n",
    "    \n",
    "    return daily_metrics\n",
    "\n",
    "# Create integrated datasets\n",
    "customer_summary = create_customer_transaction_summary()\n",
    "daily_metrics = create_daily_metrics()\n",
    "\n",
    "print(f\"Customer summary created: {customer_summary.shape}\")\n",
    "print(f\"Daily metrics created: {daily_metrics.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nCustomer Summary Sample:\")\n",
    "print(customer_summary[['customer_id', 'segment', 'total_transactions', 'total_spent', 'avg_transaction_amount']].head())\n",
    "\n",
    "print(\"\\nDaily Metrics Sample:\")\n",
    "print(daily_metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Create advanced features for analytics and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_segments():\n",
    "    \"\"\"Create RFM segmentation for customers\"\"\"\n",
    "    transactions = cleaned_data['transactions']\n",
    "    \n",
    "    # Calculate RFM metrics\n",
    "    current_date = transactions['transaction_date'].max()\n",
    "    \n",
    "    rfm = transactions.groupby('customer_id').agg({\n",
    "        'transaction_date': lambda x: (current_date - x.max()).days,  # Recency\n",
    "        'transaction_id': 'count',  # Frequency\n",
    "        'amount': 'sum'  # Monetary\n",
    "    })\n",
    "    \n",
    "    rfm.columns = ['recency', 'frequency', 'monetary']\n",
    "    \n",
    "    # Create quintiles for each metric\n",
    "    rfm['r_score'] = pd.qcut(rfm['recency'].rank(method='first'), 5, labels=[5,4,3,2,1])\n",
    "    rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    rfm['m_score'] = pd.qcut(rfm['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Combine scores\n",
    "    rfm['rfm_score'] = rfm['r_score'].astype(str) + rfm['f_score'].astype(str) + rfm['m_score'].astype(str)\n",
    "    \n",
    "    # Define segments based on RFM scores\n",
    "    def segment_customers(row):\n",
    "        if row['rfm_score'] in ['555', '554', '544', '545', '454', '455', '445']:\n",
    "            return 'Champions'\n",
    "        elif row['rfm_score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:\n",
    "            return 'Loyal Customers'\n",
    "        elif row['rfm_score'] in ['512', '511', '422', '421', '412', '411', '311']:\n",
    "            return 'Potential Loyalists'\n",
    "        elif row['rfm_score'] in ['533', '532', '531', '523', '522', '521', '515', '514', '513', '425', '424', '413', '414', '415', '315', '314', '313']:\n",
    "            return 'New Customers'\n",
    "        elif row['rfm_score'] in ['155', '154', '144', '214', '215', '115', '114']:\n",
    "            return 'At Risk'\n",
    "        elif row['rfm_score'] in ['255', '254', '245', '244', '253', '252', '243', '242', '235', '234', '225', '224']:\n",
    "            return 'Cannot Lose Them'\n",
    "        else:\n",
    "            return 'Others'\n",
    "    \n",
    "    rfm['segment'] = rfm.apply(segment_customers, axis=1)\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "def create_cohort_analysis():\n",
    "    \"\"\"Create cohort analysis data\"\"\"\n",
    "    transactions = cleaned_data['transactions']\n",
    "    \n",
    "    # Get customer's first purchase date\n",
    "    transactions['transaction_period'] = transactions['transaction_date'].dt.to_period('M')\n",
    "    \n",
    "    cohort_data = transactions.groupby('customer_id')['transaction_date'].min().reset_index()\n",
    "    cohort_data.columns = ['customer_id', 'cohort_group']\n",
    "    cohort_data['cohort_group'] = cohort_data['cohort_group'].dt.to_period('M')\n",
    "    \n",
    "    # Merge with transaction data\n",
    "    df_cohort = transactions.merge(cohort_data, on='customer_id')\n",
    "    df_cohort['period_number'] = (df_cohort['transaction_period'] - df_cohort['cohort_group']).apply(attrgetter('n'))\n",
    "    \n",
    "    # Create cohort table\n",
    "    cohort_sizes = df_cohort.groupby('cohort_group')['customer_id'].nunique().reset_index()\n",
    "    cohort_table = df_cohort.groupby(['cohort_group', 'period_number'])['customer_id'].nunique().reset_index()\n",
    "    cohort_table = cohort_table.merge(cohort_sizes, on='cohort_group')\n",
    "    cohort_table['retention_rate'] = cohort_table['customer_id_x'] / cohort_table['customer_id_y']\n",
    "    \n",
    "    return cohort_table.pivot(index='cohort_group', columns='period_number', values='retention_rate')\n",
    "\n",
    "# Create advanced features\n",
    "rfm_segments = create_customer_segments()\n",
    "\n",
    "print(\"RFM Segmentation completed!\")\n",
    "print(rfm_segments['segment'].value_counts())\n",
    "print(\"\\nRFM Sample:\")\n",
    "print(rfm_segments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Validation\n",
    "Validate transformed data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_transformed_data():\n",
    "    \"\"\"Validate quality of transformed datasets\"\"\"\n",
    "    validation_results = {}\n",
    "    \n",
    "    # Validate customer summary\n",
    "    customer_issues = []\n",
    "    if customer_summary['total_spent'].min() < 0:\n",
    "        customer_issues.append(\"Negative spend amounts found\")\n",
    "    if customer_summary['total_transactions'].isnull().sum() > 0:\n",
    "        customer_issues.append(\"NULL transaction counts found\")\n",
    "    \n",
    "    validation_results['customer_summary'] = {\n",
    "        'issues': customer_issues,\n",
    "        'records': len(customer_summary),\n",
    "        'status': 'PASS' if len(customer_issues) == 0 else 'FAIL'\n",
    "    }\n",
    "    \n",
    "    # Validate daily metrics\n",
    "    daily_issues = []\n",
    "    if daily_metrics['daily_revenue'].min() < 0:\n",
    "        daily_issues.append(\"Negative revenue found\")\n",
    "    if daily_metrics.isnull().sum().sum() > 0:\n",
    "        daily_issues.append(\"NULL values found in metrics\")\n",
    "    \n",
    "    validation_results['daily_metrics'] = {\n",
    "        'issues': daily_issues,\n",
    "        'records': len(daily_metrics),\n",
    "        'status': 'PASS' if len(daily_issues) == 0 else 'FAIL'\n",
    "    }\n",
    "    \n",
    "    # Validate RFM segments\n",
    "    rfm_issues = []\n",
    "    if rfm_segments.isnull().sum().sum() > 0:\n",
    "        rfm_issues.append(\"NULL values in RFM scores\")\n",
    "    if len(rfm_segments['segment'].unique()) < 3:\n",
    "        rfm_issues.append(\"Insufficient segment diversity\")\n",
    "    \n",
    "    validation_results['rfm_segments'] = {\n",
    "        'issues': rfm_issues,\n",
    "        'records': len(rfm_segments),\n",
    "        'status': 'PASS' if len(rfm_issues) == 0 else 'FAIL'\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "validation_results = validate_transformed_data()\n",
    "\n",
    "print(\"Data Validation Results:\")\n",
    "for dataset, results in validation_results.items():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    print(f\"  Status: {results['status']}\")\n",
    "    print(f\"  Records: {results['records']}\")\n",
    "    if results['issues']:\n",
    "        print(f\"  Issues: {', '.join(results['issues'])}\")\n",
    "    else:\n",
    "        print(\"  Issues: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "Save transformed data for analytics and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data():\n",
    "    \"\"\"Save all processed datasets\"\"\"\n",
    "    processed_path = '../data/processed/'\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    \n",
    "    # Save all cleaned datasets\n",
    "    for name, df in cleaned_data.items():\n",
    "        file_path = os.path.join(processed_path, f'{name}_cleaned.csv')\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Saved {name}_cleaned.csv\")\n",
    "    \n",
    "    # Save integrated datasets\n",
    "    customer_summary.to_csv(os.path.join(processed_path, 'customer_summary.csv'), index=False)\n",
    "    daily_metrics.to_csv(os.path.join(processed_path, 'daily_metrics.csv'), index=False)\n",
    "    rfm_segments.to_csv(os.path.join(processed_path, 'rfm_segments.csv'))\n",
    "    \n",
    "    logger.info(\"All processed data saved successfully\")\n",
    "    \n",
    "    # Create data catalog\n",
    "    catalog = {\n",
    "        'cleaned_datasets': {\n",
    "            name: {'records': len(df), 'columns': len(df.columns)} \n",
    "            for name, df in cleaned_data.items()\n",
    "        },\n",
    "        'integrated_datasets': {\n",
    "            'customer_summary': {'records': len(customer_summary), 'columns': len(customer_summary.columns)},\n",
    "            'daily_metrics': {'records': len(daily_metrics), 'columns': len(daily_metrics.columns)},\n",
    "            'rfm_segments': {'records': len(rfm_segments), 'columns': len(rfm_segments.columns)}\n",
    "        },\n",
    "        'transformation_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(processed_path, 'data_catalog.json'), 'w') as f:\n",
    "        json.dump(catalog, f, indent=2)\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "catalog = save_processed_data()\n",
    "print(\"\\nData Transformation Summary:\")\n",
    "print(f\"Cleaned datasets: {len(catalog['cleaned_datasets'])}\")\n",
    "print(f\"Integrated datasets: {len(catalog['integrated_datasets'])}\")\n",
    "print(f\"Transformation completed: {catalog['transformation_date']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}