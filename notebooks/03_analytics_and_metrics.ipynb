{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics and Business Metrics\n",
    "This notebook provides comprehensive analytics and business intelligence metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "Load the transformed datasets from the previous pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"Load processed data from CSV files\"\"\"\n",
    "    processed_path = '../data/processed/'\n",
    "    \n",
    "    # Check if processed data exists, if not use raw data\n",
    "    if not os.path.exists(processed_path):\n",
    "        print(\"Processed data not found, loading raw data...\")\n",
    "        raw_path = '../data/raw/'\n",
    "        datasets = {\n",
    "            'customers': pd.read_csv(f'{raw_path}customers.csv'),\n",
    "            'transactions': pd.read_csv(f'{raw_path}transactions.csv'),\n",
    "            'events': pd.read_csv(f'{raw_path}events.csv'),\n",
    "            'products': pd.read_csv(f'{raw_path}products.csv')\n",
    "        }\n",
    "        # Basic processing for raw data\n",
    "        datasets['transactions']['transaction_date'] = pd.to_datetime(datasets['transactions']['transaction_date'])\n",
    "        datasets['customers']['registration_date'] = pd.to_datetime(datasets['customers']['registration_date'])\n",
    "        return datasets\n",
    "    \n",
    "    datasets = {}\n",
    "    files = {\n",
    "        'customer_summary': 'customer_summary.csv',\n",
    "        'daily_metrics': 'daily_metrics.csv',\n",
    "        'rfm_segments': 'rfm_segments.csv',\n",
    "        'customers_cleaned': 'customers_cleaned.csv',\n",
    "        'transactions_cleaned': 'transactions_cleaned.csv'\n",
    "    }\n",
    "    \n",
    "    for name, filename in files.items():\n",
    "        file_path = os.path.join(processed_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            datasets[name] = pd.read_csv(file_path)\n",
    "            print(f\"Loaded {name}: {datasets[name].shape}\")\n",
    "        else:\n",
    "            print(f\"File not found: {filename}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load data\n",
    "data = load_processed_data()\n",
    "\n",
    "# Use available data\n",
    "customers = data.get('customers_cleaned', data.get('customers'))\n",
    "transactions = data.get('transactions_cleaned', data.get('transactions'))\n",
    "events = data.get('events')\n",
    "products = data.get('products')\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customer Analytics\n",
    "Analyze customer behavior, segmentation, and lifetime value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_customer_metrics():\n",
    "    \"\"\"Calculate key customer metrics\"\"\"\n",
    "    # Basic customer metrics\n",
    "    total_customers = len(customers)\n",
    "    active_customers = len(customers[customers['is_active'] == True]) if 'is_active' in customers.columns else total_customers\n",
    "    \n",
    "    # Customer acquisition by month\n",
    "    customers['registration_month'] = pd.to_datetime(customers['registration_date']).dt.to_period('M')\n",
    "    acquisition_by_month = customers.groupby('registration_month').size()\n",
    "    \n",
    "    # Customer demographics\n",
    "    age_distribution = customers['age'].describe() if 'age' in customers.columns else None\n",
    "    segment_distribution = customers['segment'].value_counts() if 'segment' in customers.columns else None\n",
    "    \n",
    "    # Transaction-based customer metrics\n",
    "    customer_txn_summary = transactions.groupby('customer_id').agg({\n",
    "        'amount': ['count', 'sum', 'mean'],\n",
    "        'transaction_date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    customer_txn_summary.columns = ['transaction_count', 'total_spent', 'avg_order_value', 'first_purchase', 'last_purchase']\n",
    "    \n",
    "    # Customer Lifetime Value (simple calculation)\n",
    "    customer_txn_summary['customer_lifetime_days'] = (\n",
    "        pd.to_datetime(customer_txn_summary['last_purchase']) - \n",
    "        pd.to_datetime(customer_txn_summary['first_purchase'])\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    customer_txn_summary['clv_estimate'] = (\n",
    "        customer_txn_summary['total_spent'] / customer_txn_summary['customer_lifetime_days'] * 365\n",
    "    ).replace([np.inf, -np.inf], customer_txn_summary['total_spent'])\n",
    "    \n",
    "    metrics = {\n",
    "        'total_customers': total_customers,\n",
    "        'active_customers': active_customers,\n",
    "        'acquisition_by_month': acquisition_by_month,\n",
    "        'age_distribution': age_distribution,\n",
    "        'segment_distribution': segment_distribution,\n",
    "        'customer_summary': customer_txn_summary\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "customer_metrics = calculate_customer_metrics()\n",
    "\n",
    "print(\"Customer Analytics:\")\n",
    "print(f\"Total Customers: {customer_metrics['total_customers']:,}\")\n",
    "print(f\"Active Customers: {customer_metrics['active_customers']:,}\")\n",
    "print(f\"\\nTop Customer Segments:\")\n",
    "if customer_metrics['segment_distribution'] is not None:\n",
    "    print(customer_metrics['segment_distribution'])\n",
    "\n",
    "print(f\"\\nCustomer Value Statistics:\")\n",
    "clv_stats = customer_metrics['customer_summary']['clv_estimate'].describe()\n",
    "print(clv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Revenue and Transaction Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue_metrics():\n",
    "    \"\"\"Calculate revenue and transaction metrics\"\"\"\n",
    "    # Overall revenue metrics\n",
    "    total_revenue = transactions['amount'].sum()\n",
    "    total_transactions = len(transactions)\n",
    "    avg_order_value = transactions['amount'].mean()\n",
    "    \n",
    "    # Daily revenue trends\n",
    "    transactions['date'] = pd.to_datetime(transactions['transaction_date']).dt.date\n",
    "    daily_revenue = transactions.groupby('date')['amount'].sum()\n",
    "    daily_transactions = transactions.groupby('date').size()\n",
    "    \n",
    "    # Monthly trends\n",
    "    transactions['month'] = pd.to_datetime(transactions['transaction_date']).dt.to_period('M')\n",
    "    monthly_revenue = transactions.groupby('month')['amount'].sum()\n",
    "    monthly_transactions = transactions.groupby('month').size()\n",
    "    \n",
    "    # Revenue by category/segment\n",
    "    category_revenue = transactions.groupby('category')['amount'].sum().sort_values(ascending=False) if 'category' in transactions.columns else None\n",
    "    \n",
    "    # Payment method analysis\n",
    "    payment_analysis = transactions.groupby('payment_method').agg({\n",
    "        'amount': ['count', 'sum', 'mean']\n",
    "    }).round(2) if 'payment_method' in transactions.columns else None\n",
    "    \n",
    "    if payment_analysis is not None:\n",
    "        payment_analysis.columns = ['transaction_count', 'total_revenue', 'avg_amount']\n",
    "    \n",
    "    metrics = {\n",
    "        'total_revenue': total_revenue,\n",
    "        'total_transactions': total_transactions,\n",
    "        'avg_order_value': avg_order_value,\n",
    "        'daily_revenue': daily_revenue,\n",
    "        'daily_transactions': daily_transactions,\n",
    "        'monthly_revenue': monthly_revenue,\n",
    "        'monthly_transactions': monthly_transactions,\n",
    "        'category_revenue': category_revenue,\n",
    "        'payment_analysis': payment_analysis\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "revenue_metrics = calculate_revenue_metrics()\n",
    "\n",
    "print(\"Revenue Analytics:\")\n",
    "print(f\"Total Revenue: ${revenue_metrics['total_revenue']:,.2f}\")\n",
    "print(f\"Total Transactions: {revenue_metrics['total_transactions']:,}\")\n",
    "print(f\"Average Order Value: ${revenue_metrics['avg_order_value']:.2f}\")\n",
    "\n",
    "print(f\"\\nMonthly Revenue Trend (Last 6 months):\")\n",
    "print(revenue_metrics['monthly_revenue'].tail(6))\n",
    "\n",
    "if revenue_metrics['category_revenue'] is not None:\n",
    "    print(f\"\\nTop Revenue Categories:\")\n",
    "    print(revenue_metrics['category_revenue'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Churn Analysis\n",
    "Analyze customer churn patterns and at-risk customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer_churn():\n",
    "    \"\"\"Analyze customer churn and identify at-risk customers\"\"\"\n",
    "    # Calculate days since last transaction\n",
    "    current_date = datetime.now()\n",
    "    last_transaction = transactions.groupby('customer_id')['transaction_date'].max()\n",
    "    last_transaction_df = pd.DataFrame({\n",
    "        'customer_id': last_transaction.index,\n",
    "        'last_transaction_date': pd.to_datetime(last_transaction.values)\n",
    "    })\n",
    "    \n",
    "    last_transaction_df['days_since_last_transaction'] = (\n",
    "        current_date - last_transaction_df['last_transaction_date']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Define churn thresholds (customizable based on business)\n",
    "    churn_threshold_days = 90\n",
    "    at_risk_threshold_days = 60\n",
    "    \n",
    "    # Classify customers\n",
    "    def classify_customer_status(days):\n",
    "        if days > churn_threshold_days:\n",
    "            return 'Churned'\n",
    "        elif days > at_risk_threshold_days:\n",
    "            return 'At Risk'\n",
    "        elif days > 30:\n",
    "            return 'Declining'\n",
    "        else:\n",
    "            return 'Active'\n",
    "    \n",
    "    last_transaction_df['churn_status'] = last_transaction_df['days_since_last_transaction'].apply(classify_customer_status)\n",
    "    \n",
    "    # Calculate churn rate\n",
    "    churn_summary = last_transaction_df['churn_status'].value_counts()\n",
    "    total_customers_with_transactions = len(last_transaction_df)\n",
    "    churn_rate = (churn_summary.get('Churned', 0) / total_customers_with_transactions) * 100\n",
    "    at_risk_rate = (churn_summary.get('At Risk', 0) / total_customers_with_transactions) * 100\n",
    "    \n",
    "    # Identify high-value churned customers\n",
    "    customer_value = transactions.groupby('customer_id')['amount'].sum()\n",
    "    churn_analysis = last_transaction_df.merge(\n",
    "        customer_value.reset_index().rename(columns={'amount': 'total_spent'}),\n",
    "        on='customer_id'\n",
    "    )\n",
    "    \n",
    "    high_value_churned = churn_analysis[\n",
    "        (churn_analysis['churn_status'] == 'Churned') & \n",
    "        (churn_analysis['total_spent'] > churn_analysis['total_spent'].quantile(0.75))\n",
    "    ]\n",
    "    \n",
    "    # Monthly churn trend\n",
    "    churn_analysis['churn_month'] = churn_analysis['last_transaction_date'].dt.to_period('M')\n",
    "    monthly_churn = churn_analysis[churn_analysis['churn_status'] == 'Churned'].groupby('churn_month').size()\n",
    "    \n",
    "    results = {\n",
    "        'churn_summary': churn_summary,\n",
    "        'churn_rate': churn_rate,\n",
    "        'at_risk_rate': at_risk_rate,\n",
    "        'high_value_churned': high_value_churned,\n",
    "        'monthly_churn_trend': monthly_churn,\n",
    "        'customer_status_data': churn_analysis\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "churn_analysis = analyze_customer_churn()\n",
    "\n",
    "print(\"Churn Analysis:\")\n",
    "print(f\"Overall Churn Rate: {churn_analysis['churn_rate']:.2f}%\")\n",
    "print(f\"At-Risk Customer Rate: {churn_analysis['at_risk_rate']:.2f}%\")\n",
    "print(f\"\\nCustomer Status Distribution:\")\n",
    "print(churn_analysis['churn_summary'])\n",
    "print(f\"\\nHigh-Value Churned Customers: {len(churn_analysis['high_value_churned'])}\")\n",
    "if len(churn_analysis['high_value_churned']) > 0:\n",
    "    total_lost_revenue = churn_analysis['high_value_churned']['total_spent'].sum()\n",
    "    print(f\"Revenue at Risk from High-Value Churned: ${total_lost_revenue:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection\n",
    "Identify anomalies in transaction patterns and customer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_transaction_anomalies():\n",
    "    \"\"\"Detect anomalies in transaction data\"\"\"\n",
    "    # Daily transaction volume anomalies\n",
    "    daily_stats = revenue_metrics['daily_transactions']\n",
    "    mean_daily = daily_stats.mean()\n",
    "    std_daily = daily_stats.std()\n",
    "    \n",
    "    # Define anomalies as transactions beyond 2 standard deviations\n",
    "    anomaly_threshold = 2\n",
    "    upper_bound = mean_daily + (anomaly_threshold * std_daily)\n",
    "    lower_bound = max(0, mean_daily - (anomaly_threshold * std_daily))\n",
    "    \n",
    "    daily_anomalies = daily_stats[\n",
    "        (daily_stats > upper_bound) | (daily_stats < lower_bound)\n",
    "    ]\n",
    "    \n",
    "    # Transaction amount anomalies\n",
    "    amount_mean = transactions['amount'].mean()\n",
    "    amount_std = transactions['amount'].std()\n",
    "    amount_upper = amount_mean + (3 * amount_std)  # 3 sigma for amounts\n",
    "    \n",
    "    large_transactions = transactions[transactions['amount'] > amount_upper]\n",
    "    \n",
    "    # Customer behavior anomalies\n",
    "    customer_daily_spend = transactions.groupby(['customer_id', 'date'])['amount'].sum().reset_index()\n",
    "    customer_avg_daily = customer_daily_spend.groupby('customer_id')['amount'].mean()\n",
    "    customer_std_daily = customer_daily_spend.groupby('customer_id')['amount'].std().fillna(0)\n",
    "    \n",
    "    # Find customers with unusual spending patterns\n",
    "    unusual_customers = []\n",
    "    for customer_id in customer_avg_daily.index:\n",
    "        avg_spend = customer_avg_daily[customer_id]\n",
    "        std_spend = customer_std_daily[customer_id]\n",
    "        if std_spend > 0 and (std_spend / avg_spend) > 2:  # High variability\n",
    "            unusual_customers.append(customer_id)\n",
    "    \n",
    "    # Frequency anomalies (customers with sudden increase in transaction frequency)\n",
    "    transactions['week'] = pd.to_datetime(transactions['transaction_date']).dt.to_period('W')\n",
    "    weekly_customer_txns = transactions.groupby(['customer_id', 'week']).size().reset_index(name='weekly_count')\n",
    "    \n",
    "    customer_weekly_avg = weekly_customer_txns.groupby('customer_id')['weekly_count'].mean()\n",
    "    high_frequency_customers = customer_weekly_avg[customer_weekly_avg > customer_weekly_avg.quantile(0.95)].index.tolist()\n",
    "    \n",
    "    anomalies = {\n",
    "        'daily_volume_anomalies': daily_anomalies,\n",
    "        'large_transactions': large_transactions,\n",
    "        'unusual_spending_customers': unusual_customers[:10],  # Top 10\n",
    "        'high_frequency_customers': high_frequency_customers[:10],  # Top 10\n",
    "        'anomaly_stats': {\n",
    "            'daily_mean': mean_daily,\n",
    "            'daily_std': std_daily,\n",
    "            'amount_threshold': amount_upper,\n",
    "            'total_anomalous_days': len(daily_anomalies),\n",
    "            'total_large_transactions': len(large_transactions)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "anomalies = detect_transaction_anomalies()\n",
    "\n",
    "print(\"Anomaly Detection Results:\")\n",
    "print(f\"Anomalous Days (Transaction Volume): {anomalies['anomaly_stats']['total_anomalous_days']}\")\n",
    "print(f\"Large Transactions (>${anomalies['anomaly_stats']['amount_threshold']:.2f}+): {anomalies['anomaly_stats']['total_large_transactions']}\")\n",
    "print(f\"Customers with Unusual Spending: {len(anomalies['unusual_spending_customers'])}\")\n",
    "print(f\"High-Frequency Customers: {len(anomalies['high_frequency_customers'])}\")\n",
    "\n",
    "if len(anomalies['large_transactions']) > 0:\n",
    "    print(\"\\nTop 5 Largest Transactions:\")\n",
    "    top_transactions = anomalies['large_transactions'].nlargest(5, 'amount')[['transaction_id', 'customer_id', 'amount', 'transaction_date']]\n",
    "    print(top_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Intelligence Dashboard Data\n",
    "Prepare data for dashboard visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard_data():\n",
    "    \"\"\"Create summarized data for business intelligence dashboard\"\"\"\n",
    "    \n",
    "    # KPI Summary\n",
    "    kpi_summary = {\n",
    "        'total_customers': customer_metrics['total_customers'],\n",
    "        'active_customers': customer_metrics['active_customers'],\n",
    "        'total_revenue': revenue_metrics['total_revenue'],\n",
    "        'total_transactions': revenue_metrics['total_transactions'],\n",
    "        'avg_order_value': revenue_metrics['avg_order_value'],\n",
    "        'churn_rate': churn_analysis['churn_rate'],\n",
    "        'at_risk_customers': churn_analysis['churn_summary'].get('At Risk', 0),\n",
    "        'churned_customers': churn_analysis['churn_summary'].get('Churned', 0)\n",
    "    }\n",
    "    \n",
    "    # Weekly trends for charts\n",
    "    transactions['week'] = pd.to_datetime(transactions['transaction_date']).dt.to_period('W')\n",
    "    weekly_trends = transactions.groupby('week').agg({\n",
    "        'amount': ['count', 'sum'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    weekly_trends.columns = ['transactions', 'revenue', 'unique_customers']\n",
    "    weekly_trends = weekly_trends.reset_index()\n",
    "    weekly_trends['week'] = weekly_trends['week'].astype(str)\n",
    "    \n",
    "    # Customer segmentation data\n",
    "    segment_data = customer_metrics['segment_distribution'].to_dict() if customer_metrics['segment_distribution'] is not None else {}\n",
    "    \n",
    "    # Top customers by value\n",
    "    top_customers = customer_metrics['customer_summary'].nlargest(10, 'total_spent')[['total_spent', 'transaction_count', 'avg_order_value']]\n",
    "    top_customers = top_customers.reset_index()\n",
    "    \n",
    "    # Alert data\n",
    "    alerts = {\n",
    "        'high_churn_rate': churn_analysis['churn_rate'] > 20,\n",
    "        'large_transactions_today': len(anomalies['large_transactions']) > 0,\n",
    "        'unusual_customer_behavior': len(anomalies['unusual_spending_customers']) > 5,\n",
    "        'revenue_decline': len(revenue_metrics['daily_revenue']) > 7 and revenue_metrics['daily_revenue'].tail(7).mean() < revenue_metrics['daily_revenue'].head(-7).mean()\n",
    "    }\n",
    "    \n",
    "    dashboard_data = {\n",
    "        'kpi_summary': kpi_summary,\n",
    "        'weekly_trends': weekly_trends.to_dict('records'),\n",
    "        'customer_segments': segment_data,\n",
    "        'top_customers': top_customers.to_dict('records'),\n",
    "        'alerts': alerts,\n",
    "        'last_updated': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return dashboard_data\n",
    "\n",
    "dashboard_data = create_dashboard_data()\n",
    "\n",
    "print(\"Dashboard Data Summary:\")\n",
    "print(\"KPIs:\")\n",
    "for key, value in dashboard_data['kpi_summary'].items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        if 'rate' in key or 'churn' in key:\n",
    "            print(f\"  {key}: {value:.2f}%\")\n",
    "        elif 'revenue' in key or 'value' in key:\n",
    "            print(f\"  {key}: ${value:,.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:,}\")\n",
    "\n",
    "print(f\"\\nActive Alerts:\")\n",
    "active_alerts = [alert for alert, status in dashboard_data['alerts'].items() if status]\n",
    "if active_alerts:\n",
    "    for alert in active_alerts:\n",
    "        print(f\"  ⚠️ {alert.replace('_', ' ').title()}\")\n",
    "else:\n",
    "    print(\"  ✅ No active alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Analytics Results\n",
    "Save all analytics results for API consumption and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analytics_results():\n",
    "    \"\"\"Export all analytics results to files\"\"\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    analytics_path = '../data/processed/analytics/'\n",
    "    os.makedirs(analytics_path, exist_ok=True)\n",
    "    \n",
    "    # Export customer metrics\n",
    "    customer_metrics['customer_summary'].to_csv(f'{analytics_path}customer_analysis.csv')\n",
    "    \n",
    "    # Export churn analysis\n",
    "    churn_analysis['customer_status_data'].to_csv(f'{analytics_path}churn_analysis.csv', index=False)\n",
    "    \n",
    "    # Export anomalies\n",
    "    if len(anomalies['large_transactions']) > 0:\n",
    "        anomalies['large_transactions'].to_csv(f'{analytics_path}transaction_anomalies.csv', index=False)\n",
    "    \n",
    "    # Export dashboard data as JSON\n",
    "    with open(f'{analytics_path}dashboard_data.json', 'w') as f:\n",
    "        json.dump(dashboard_data, f, indent=2, default=str)\n",
    "    \n",
    "    # Create analytics summary report\n",
    "    summary_report = {\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'data_period': {\n",
    "            'start_date': str(pd.to_datetime(transactions['transaction_date']).min().date()),\n",
    "            'end_date': str(pd.to_datetime(transactions['transaction_date']).max().date())\n",
    "        },\n",
    "        'key_insights': {\n",
    "            'total_customers': customer_metrics['total_customers'],\n",
    "            'total_revenue': f\"${revenue_metrics['total_revenue']:,.2f}\",\n",
    "            'churn_rate': f\"{churn_analysis['churn_rate']:.2f}%\",\n",
    "            'avg_order_value': f\"${revenue_metrics['avg_order_value']:.2f}\",\n",
    "            'anomalies_detected': anomalies['anomaly_stats']['total_large_transactions']\n",
    "        },\n",
    "        'recommendations': [\n",
    "            \"Focus on high-value customers at risk of churning\",\n",
    "            \"Investigate unusual transaction patterns for fraud detection\",\n",
    "            \"Implement retention campaigns for at-risk customers\",\n",
    "            \"Optimize customer acquisition for high-value segments\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f'{analytics_path}analytics_summary.json', 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    print(f\"Analytics results exported to {analytics_path}\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"  - customer_analysis.csv\")\n",
    "    print(\"  - churn_analysis.csv\")\n",
    "    print(\"  - transaction_anomalies.csv\")\n",
    "    print(\"  - dashboard_data.json\")\n",
    "    print(\"  - analytics_summary.json\")\n",
    "    \n",
    "    return summary_report\n",
    "\n",
    "summary_report = export_analytics_results()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYTICS PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Analysis Period: {summary_report['data_period']['start_date']} to {summary_report['data_period']['end_date']}\")\n",
    "print(f\"Generated: {summary_report['generated_at']}\")\n",
    "print(\"\\nKey Business Insights:\")\n",
    "for insight, value in summary_report['key_insights'].items():\n",
    "    print(f\"  {insight.replace('_', ' ').title()}: {value}\")\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, rec in enumerate(summary_report['recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.0"\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}